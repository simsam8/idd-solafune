{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is a part of motokimura's baseline solution for the [Solafune Identifying Deforestation Drivers competition](https://solafune.com/competitions/68ad4759-4686-4bb3-94b8-7063f755b43d?menu=about&tab=overview).\n",
    "See https://github.com/motokimura/solafune_deforestation_baseline for the complete code.\n",
    "\n",
    "> cf. @solafune (https://solafune.com) Use for any purpose other than participation in the competition or commercial use is prohibited. If you would like to use them for any of the above purposes, please contact us.\n",
    "\n",
    "### Description\n",
    "\n",
    "**By running this notebook, you will achieve a score of around 0.533 on the public leaderboard.**\n",
    "\n",
    "This notebook trains a U-Net model for 4-class segmentation (`grassland_shrubland`, `logging`, `mining`, and `plantation`) and generates a submission JSON file for the evaluation images from the output from the U-Net model.\n",
    "\n",
    "The submission JSON file is saved to `data/submission.json`.\n",
    "\n",
    "Before running this notebook, you have to run `generate_masks.ipynb` to generate `.npy` files used for training\n",
    "(`generate_masks.ipynb` is available from https://github.com/motokimura/solafune_deforestation_baseline).\n",
    "\n",
    "### Requirements\n",
    "\n",
    "#### Datasets\n",
    "\n",
    "Organize the dataset as follows:\n",
    "\n",
    "```\n",
    "data/\n",
    "├── evaluation_images/\n",
    "│   ├── evaluation_0.tif\n",
    "│   ├── evaluation_1.tif\n",
    "│   ├── evaluation_2.tif\n",
    "│   ├── ...\n",
    "├── train_images/\n",
    "│   ├── train_0.tif\n",
    "│   ├── train_1.tif\n",
    "│   ├── train_2.tif\n",
    "│   ├── ...\n",
    "├── train_masks/\n",
    "│   ├── train_0.npy\n",
    "│   ├── train_1.npy\n",
    "│   ├── train_2.npy\n",
    "│   ├── ...\n",
    "```\n",
    "\n",
    "`evaluation_images` and `train_images` can be downloaded from the competition page.\n",
    "\n",
    "`train_masks` can be generated by running `generate_masks.ipynb` available from https://github.com/motokimura/solafune_deforestation_baseline.\n",
    "\n",
    "#### Libraries\n",
    "\n",
    "Please install the python packages imported the cell below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import albumentations as albu  # tested with 1.4.24\n",
    "import imagecodecs\n",
    "import numpy as np  # tested with 1.26.4\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl  # tested with 2.5.0.post0\n",
    "import segmentation_models_pytorch as smp  # tested with 0.3.4\n",
    "import sklearn\n",
    "import tensorboard\n",
    "import tifffile\n",
    "import timm  # tested with 0.9.7\n",
    "import torch  # tested with 2.5.1\n",
    "\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from rasterio import features\n",
    "from shapely.geometry import Polygon, shape\n",
    "from skimage import measure\n",
    "from timm.optim import create_optimizer_v2\n",
    "from timm.scheduler import create_scheduler_v2\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = Path(\"./data\")\n",
    "\n",
    "class_names = [\"grassland_shrubland\", \"logging\", \"mining\", \"plantation\"]\n",
    "\n",
    "epochs = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define dataset class to load images and masks for training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mask(mask_path):\n",
    "    mask = np.load(mask_path)  # (4, H, W), uint8\n",
    "    assert mask.shape == (4, 1024, 1024)\n",
    "    mask = mask.transpose(1, 2, 0)  # (H, W, 4)\n",
    "    return mask.astype(np.float32) / 255.0  # normalize to [0, 1]\n",
    "\n",
    "\n",
    "def load_image(image_path):\n",
    "    image = tifffile.imread(image_path)  # (H, W, 12), float64\n",
    "    assert image.shape == (1024, 1024, 12)\n",
    "    image = np.nan_to_num(image)  # replace NaN with 0\n",
    "    return image.astype(np.float32)\n",
    "\n",
    "\n",
    "def normalize_image(image):\n",
    "    # mean of train images\n",
    "    mean = np.array(\n",
    "        [\n",
    "            285.8190561180765,\n",
    "            327.22091430696577,\n",
    "            552.9305957826701,\n",
    "            392.1575148484924,\n",
    "            914.3138803812591,\n",
    "            2346.1184507500043,\n",
    "            2884.4831706095824,\n",
    "            2886.442429854111,\n",
    "            3176.7501338557763,\n",
    "            3156.934442092072,\n",
    "            1727.1940075511282,\n",
    "            848.573373995044,\n",
    "        ],\n",
    "        dtype=np.float32\n",
    "    )\n",
    "\n",
    "    # std of train images\n",
    "    std = np.array(\n",
    "        [\n",
    "            216.44975668759372,\n",
    "            269.8880248304874,\n",
    "            309.92790753407064,\n",
    "            397.45655590699,\n",
    "            400.22078920482215,\n",
    "            630.3269651264278,\n",
    "            789.8006920468097,\n",
    "            810.4773696969773,\n",
    "            852.9031432100967,\n",
    "            807.5976198303886,\n",
    "            631.7808113929271,\n",
    "            502.66788721341396,\n",
    "        ],\n",
    "        dtype=np.float32\n",
    "    )\n",
    "    \n",
    "    mean = mean.reshape(12, 1, 1)\n",
    "    std = std.reshape(12, 1, 1)\n",
    "\n",
    "    return (image - mean) / std\n",
    "\n",
    "\n",
    "class TrainValDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_root, sample_indices, augmentations=None):\n",
    "        self.image_paths, self.mask_paths = [], []\n",
    "        for i in sample_indices:\n",
    "            self.image_paths.append(data_root / \"train_images\" / f\"train_{i}.tif\")\n",
    "            self.mask_paths.append(data_root / \"train_masks\" / f\"train_{i}.npy\")\n",
    "        self.augmentations = augmentations\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = {\n",
    "            \"image\": load_image(self.image_paths[idx]),\n",
    "            \"mask\": load_mask(self.mask_paths[idx]),\n",
    "        }\n",
    "\n",
    "        if self.augmentations is not None:\n",
    "            sample = self.augmentations(**sample)\n",
    "\n",
    "        sample[\"image\"] = sample[\"image\"].transpose(2, 0, 1)  # (12, H, W)\n",
    "        sample[\"mask\"] = sample[\"mask\"].transpose(2, 0, 1)  # (4, H, W)\n",
    "\n",
    "        sample[\"image\"] = normalize_image(sample[\"image\"])\n",
    "\n",
    "        # add metadata\n",
    "        sample[\"image_path\"] = str(self.image_paths[idx])\n",
    "        sample[\"mask_path\"] = str(self.mask_paths[idx])\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define U-Net model using pytorch-lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # prepare segmentation model\n",
    "        self.model = smp.create_model(\n",
    "            arch=\"unet\",\n",
    "            encoder_name=\"tu-tf_efficientnetv2_s\",  # use `tf_efficientnetv2_s` from timm\n",
    "            encoder_weights=\"imagenet\",  # always starts from imagenet pre-trained weight\n",
    "            in_channels=12,\n",
    "            classes=4,\n",
    "        )\n",
    "\n",
    "        # prepare loss functions\n",
    "        self.dice_loss_fn = smp.losses.DiceLoss(mode=smp.losses.MULTILABEL_MODE, from_logits=True)\n",
    "        self.bce_loss_fn = smp.losses.SoftBCEWithLogitsLoss(smooth_factor=0.0)\n",
    "\n",
    "        self.training_step_outputs = []\n",
    "        self.validation_step_outputs = []\n",
    "    \n",
    "    def forward(self, image):\n",
    "        # assuming image is already normalized\n",
    "        return self.model(image)  # logits\n",
    "\n",
    "    def shared_step(self, batch, stage):\n",
    "        image = batch[\"image\"]\n",
    "        mask = batch[\"mask\"]\n",
    "\n",
    "        logits_mask = self.forward(image)\n",
    "\n",
    "        loss = self.dice_loss_fn(logits_mask, mask) + self.bce_loss_fn(logits_mask, mask)\n",
    "\n",
    "        # count tp, fp, fn, tn for each class to compute validation metrics at the end of epoch\n",
    "        thresh = 0.5\n",
    "        prob_mask = logits_mask.sigmoid()\n",
    "        tp, fp, fn, tn = smp.metrics.get_stats(\n",
    "            (prob_mask > thresh).long(),\n",
    "            mask.long(),\n",
    "            mode=smp.losses.MULTILABEL_MODE,\n",
    "        )  # each of tp, fp, fn, tn is a tensor of shape (batch_size, num_classes) and of type long\n",
    "\n",
    "        output = {\n",
    "            \"loss\": loss.detach().cpu(),\n",
    "            \"tp\": tp.detach().cpu(),\n",
    "            \"fp\": fp.detach().cpu(),\n",
    "            \"fn\": fn.detach().cpu(),\n",
    "            \"tn\": tn.detach().cpu(),\n",
    "        }\n",
    "        if stage == \"train\":\n",
    "            self.training_step_outputs.append(output)\n",
    "        else:\n",
    "            self.validation_step_outputs.append(output)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self.shared_step(batch, \"train\")\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self.shared_step(batch, \"val\")\n",
    "\n",
    "    def shared_epoch_end(self, outputs, stage):\n",
    "        def log(name, tensor, prog_bar=False):\n",
    "            self.log(f\"{stage}/{name}\", tensor.to(self.device), sync_dist=True, prog_bar=prog_bar)\n",
    "\n",
    "        # aggregate loss\n",
    "        loss = torch.stack([x[\"loss\"] for x in outputs]).mean()\n",
    "        log(\"loss\", loss, prog_bar=True)\n",
    "\n",
    "        # aggregate tp, fp, fn, tn to compose F1 score for each class\n",
    "        tp = torch.cat([x[\"tp\"] for x in outputs])\n",
    "        fp = torch.cat([x[\"fp\"] for x in outputs])\n",
    "        fn = torch.cat([x[\"fn\"] for x in outputs])\n",
    "        tn = torch.cat([x[\"tn\"] for x in outputs])\n",
    "\n",
    "        f1_scores = {}\n",
    "        for i, class_name in enumerate(class_names):\n",
    "            f1_scores[class_name] = smp.metrics.f1_score(tp[:, i], fp[:, i], fn[:, i], tn[:, i], reduction=\"macro-imagewise\")\n",
    "            log(f\"f1/{class_name}\", f1_scores[class_name], prog_bar=False)\n",
    "\n",
    "        f1_avg = torch.stack([v for v in f1_scores.values()]).mean()\n",
    "        log(\"f1\", f1_avg, prog_bar=True)\n",
    "    \n",
    "    def on_train_epoch_end(self):\n",
    "        self.shared_epoch_end(self.training_step_outputs, \"train\")\n",
    "        self.training_step_outputs.clear()\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        self.shared_epoch_end(self.validation_step_outputs, \"val\")\n",
    "        self.validation_step_outputs.clear()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # optimizer\n",
    "        optimizer = create_optimizer_v2(\n",
    "            self.parameters(),\n",
    "            opt=\"adamw\",\n",
    "            lr=1e-4,\n",
    "            weight_decay=1e-2,\n",
    "            filter_bias_and_bn=True,  # filter out bias and batchnorm from weight decay\n",
    "        )\n",
    "\n",
    "        # lr scheduler\n",
    "        scheduler, _ = create_scheduler_v2(\n",
    "            optimizer,\n",
    "            sched=\"cosine\",\n",
    "            num_epochs=epochs,\n",
    "            min_lr=0.0,\n",
    "            warmup_lr=1e-5,\n",
    "            warmup_epochs=0,\n",
    "            warmup_prefix=False,\n",
    "            step_on_epochs=True,\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"interval\": \"epoch\",\n",
    "            },\n",
    "        }\n",
    "\n",
    "    def lr_scheduler_step(self, scheduler, metric):\n",
    "        # workaround for timm's scheduler:\n",
    "        # https://github.com/Lightning-AI/lightning/issues/5555#issuecomment-1065894281\n",
    "        scheduler.step(epoch=self.current_epoch)  # timm's scheduler need the epoch value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare trainer of pytorch-lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_output_dir = data_root / \"training_result\"\n",
    "\n",
    "# split train_images into train-set and val-set\n",
    "sample_indices = list(range(176))  # train_0.tif to train_175.tif\n",
    "train_indices, val_indices = sklearn.model_selection.train_test_split(sample_indices, test_size=0.2, random_state=42)\n",
    "\n",
    "# augmentations applied only to train-set\n",
    "augmentations = albu.Compose(\n",
    "    [\n",
    "        # shift, scale, and rotate\n",
    "        albu.ShiftScaleRotate(\n",
    "            p=0.5,\n",
    "            shift_limit=0.0625,\n",
    "            scale_limit=0.1,\n",
    "            rotate_limit=15,\n",
    "            border_mode=0,  # constant border\n",
    "            value=0,\n",
    "            mask_value=0,\n",
    "            interpolation=2,  # bicubic\n",
    "        ),\n",
    "        # random crop\n",
    "        albu.RandomCrop(\n",
    "            p=1,\n",
    "            width=512,\n",
    "            height=512,\n",
    "        ),\n",
    "        # flip, transpose, and rotate90\n",
    "        albu.HorizontalFlip(p=0.5),\n",
    "        albu.VerticalFlip(p=0.5),\n",
    "        albu.Transpose(p=0.5),\n",
    "        albu.RandomRotate90(p=0.5),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# prepare data loaders\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    TrainValDataset(\n",
    "        data_root,\n",
    "        train_indices,\n",
    "        augmentations=augmentations,\n",
    "    ),\n",
    "    batch_size=8,\n",
    "    num_workers=8,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    TrainValDataset(\n",
    "        data_root,\n",
    "        val_indices,\n",
    "        augmentations=None,\n",
    "    ),\n",
    "    batch_size=2,\n",
    "    num_workers=8,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "# prepare trainer\n",
    "trainer = Trainer(\n",
    "    max_epochs=epochs,\n",
    "    callbacks = [\n",
    "        # save model with best validation F1 score\n",
    "        ModelCheckpoint(\n",
    "            dirpath=train_output_dir,\n",
    "            filename=\"best_f1_05\",\n",
    "            save_weights_only=True,\n",
    "            save_top_k=1,\n",
    "            monitor=\"val/f1\",\n",
    "            mode=\"max\",\n",
    "            save_last=False,\n",
    "        ),\n",
    "        LearningRateMonitor(logging_interval=\"step\"),\n",
    "    ],\n",
    "    logger=[TensorBoardLogger(train_output_dir, name=None)],\n",
    "    precision=\"16-mixed\",\n",
    "    deterministic=True,\n",
    "    benchmark=False,\n",
    "    sync_batchnorm=False,\n",
    "    check_val_every_n_epoch=5,\n",
    "    default_root_dir=os.getcwd(),\n",
    "    accelerator=\"auto\",\n",
    "    devices='auto',\n",
    "    strategy=\"auto\",\n",
    "    log_every_n_steps=5,\n",
    ")\n",
    "\n",
    "# prepare model\n",
    "model = Model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start training!\n",
    "\n",
    "With the default setting, 10 GB of GPU memory is required. To reduce the memory usage, you can decrease the batch size.\n",
    "\n",
    "The trained model is saved as `data/training_result/best_f1_05.ckpt`.\n",
    "\n",
    "Tensorboard logs are also saved under `data/training_result/version_xx`.\n",
    "\n",
    "**The execution often does not finish even after reaching 200 epochs. In that case, you can stop the execution manually and just proceed to the next cell (do not restart the notebook!).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start training\n",
    "trainer.fit(\n",
    "    model,\n",
    "    train_dataloaders=train_loader,\n",
    "    val_dataloaders=val_loader,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the evaluation metric for the validation set\n",
    "\n",
    "Before predicting the evaluation images, let's predict the validation set and compute the evaluation metric.\n",
    "\n",
    "This may be useful to check if the model is trained properly and to tune the parameters for post-processing (e.g., score threshold, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(model, loader, pred_output_dir):\n",
    "    pred_output_dir = Path(pred_output_dir)\n",
    "    pred_output_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    for batch in tqdm(loader):\n",
    "        img = batch[\"image\"].cuda()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits_mask = model(img)\n",
    "            prob_mask = logits_mask.sigmoid()\n",
    "\n",
    "        # save prob mask as numpy array\n",
    "        for i in range(img.size(0)):\n",
    "            file_name = os.path.basename(batch[\"image_path\"][i])\n",
    "            prob_mask_i = prob_mask[i].cpu().numpy()  # (4, 1024, 1024)\n",
    "\n",
    "            np.save(\n",
    "                pred_output_dir / file_name.replace(\".tif\", \".npy\"),\n",
    "                prob_mask_i.astype(np.float16),\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best checkpoint and run inference on val-set\n",
    "del model\n",
    "\n",
    "model = Model()\n",
    "model.load_state_dict(torch.load(train_output_dir / \"best_f1_05.ckpt\")[\"state_dict\"])\n",
    "model = model.cuda()\n",
    "model.eval()\n",
    "\n",
    "val_pred_dir = data_root / \"val_preds\"\n",
    "run_inference(model, val_loader, val_pred_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_f1_score(pred_mask, truth_mask):\n",
    "    # `pred_mask` is a binary numpy array of shape (H, W) = (1024, 1024)\n",
    "    # `truth_mask` is a binaru numpy array of shape (H, W) = (1024, 1024)\n",
    "    assert pred_mask.shape == (1024, 1024), f\"{pred_mask.shape=}\"\n",
    "    assert truth_mask.shape == (1024, 1024), f\"{truth_mask.shape=}\"\n",
    "\n",
    "    tp = ((pred_mask > 0) & (truth_mask > 0)).sum()\n",
    "    fp = ((pred_mask > 0) & (truth_mask == 0)).sum()\n",
    "    fn = ((pred_mask == 0) & (truth_mask > 0)).sum()\n",
    "    precision = tp / (tp + fp) if tp + fp > 0 else 1  # if no prediction, precision is considered as 1\n",
    "    recall = tp / (tp + fn) if tp + fn > 0 else 1  # if no ground truth, recall is considered as 1\n",
    "    f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0  # if either precision or recall is 0, f1 is 0\n",
    "\n",
    "    return f1\n",
    "\n",
    "\n",
    "score_thresh = 0.5  # threshold to binarize the prediction mask\n",
    "min_area = 10000  # if the predicted area of a class is less than this, submit an zero mask because small predicted areas are often false positives\n",
    "\n",
    "val_f1_scores = {}\n",
    "for idx in sorted(val_indices):\n",
    "    fn = f\"train_{idx}\"\n",
    "    # prepare prediction mask\n",
    "    pred_mask = np.load(val_pred_dir / f\"{fn}.npy\")  # (4, 1024, 1024)\n",
    "    pred_mask = pred_mask > score_thresh  # binarize\n",
    "    # prepare ground truth mask\n",
    "    truth_mask = np.load(data_root / \"train_masks\" / f\"{fn}.npy\")  # (4, 1024, 1024)\n",
    "    # compute f1 score for each class\n",
    "    val_f1_scores[fn] = {}\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        pred_for_a_class = pred_mask[i]\n",
    "        if pred_for_a_class.sum() < min_area:\n",
    "            pred_for_a_class = np.zeros_like(pred_for_a_class)  # set all to zero if the predicted area is less than `min_area`\n",
    "        val_f1_scores[fn][class_name] = compute_f1_score(pred_for_a_class, truth_mask[i])\n",
    "val_f1_scores = pd.DataFrame(val_f1_scores).T\n",
    "\n",
    "# add a column for average of all the 4 classes\n",
    "val_f1_scores[\"all_classes\"] = val_f1_scores.mean(axis=1)\n",
    "# add a row for average of all the val images\n",
    "val_f1_scores.loc[\"all_images\"] = val_f1_scores.mean()\n",
    "\n",
    "print(f\"val f1 score: {val_f1_scores.loc['all_images', 'all_classes']:.4f}\")\n",
    "\n",
    "val_f1_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict the evaluation images and generate a submission JSON file\n",
    "\n",
    "Let's predict the evaluation images as already done with the validation set, and generate a submission JSON file.\n",
    "\n",
    "The submission JSON file will be saved as `data/submission.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_root):\n",
    "        self.image_paths = []\n",
    "        for i in range(118):  # evaluation_0.tif to evaluation_117.tif\n",
    "            self.image_paths.append(data_root / \"evaluation_images\" / f\"evaluation_{i}.tif\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = {\n",
    "            \"image\": load_image(self.image_paths[idx]),\n",
    "        }\n",
    "\n",
    "        sample[\"image\"] = sample[\"image\"].transpose(2, 0, 1)  # (12, H, W)\n",
    "        sample[\"image\"] = normalize_image(sample[\"image\"])\n",
    "\n",
    "        # add metadata\n",
    "        sample[\"image_path\"] = str(self.image_paths[idx])\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(\n",
    "    TestDataset(data_root),\n",
    "    batch_size=4,\n",
    "    num_workers=8,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "test_pred_dir = data_root / \"test_preds\"\n",
    "run_inference(model, test_loader, test_pred_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`detect_polygons()` below extracts isolated areas as polygons from the predicted mask.\n",
    "\n",
    "The point is `min_area` parameter to filter out small areas. Small predicted areas are often false positives which decrease the evaluation score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_polygons(pred_dir, score_thresh, min_area):\n",
    "    pred_dir = Path(pred_dir)\n",
    "    pred_paths = list(pred_dir.glob(\"*.npy\"))\n",
    "    pred_paths = sorted(pred_paths)\n",
    "\n",
    "    polygons_all_imgs = {}\n",
    "    for pred_path in tqdm(pred_paths):\n",
    "        polygons_all_classes = {}\n",
    "\n",
    "        mask = np.load(pred_path)  # (4, 1024, 1024)\n",
    "        mask = mask > score_thresh  # binarize\n",
    "        for i, class_name in enumerate(class_names):\n",
    "            mask_for_a_class = mask[i]\n",
    "            if mask_for_a_class.sum() < min_area:\n",
    "                mask_for_a_class = np.zeros_like(mask_for_a_class)  # set all to zero if the predicted area is less than `min_area`\n",
    "\n",
    "            # extract polygons from the binarized mask\n",
    "            label = measure.label(mask_for_a_class, connectivity=2, background=0).astype(np.uint8)\n",
    "            polygons = []\n",
    "            for p, value in features.shapes(label, label):\n",
    "                p = shape(p).buffer(0.5)\n",
    "                p = p.simplify(tolerance=0.5)\n",
    "                polygons.append(p)\n",
    "            polygons_all_classes[class_name] = polygons\n",
    "        polygons_all_imgs[pred_path.name.replace(\".npy\", \".tif\")] = polygons_all_classes\n",
    "\n",
    "    return polygons_all_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_polygons = detect_polygons(test_pred_dir, score_thresh=score_thresh, min_area=min_area)\n",
    "\n",
    "submission_save_path = data_root / f\"submission.json\"\n",
    "\n",
    "images = []\n",
    "for img_id in range(118):  # evaluation_0.tif to evaluation_117.tif\n",
    "    annotations = []\n",
    "    for class_name in class_names:\n",
    "        for poly in test_pred_polygons[f\"evaluation_{img_id}.tif\"][class_name]:\n",
    "            seg: list[float] = []  # [x0, y0, x1, y1, ..., xN, yN]\n",
    "            for xy in poly.exterior.coords:\n",
    "                seg.extend(xy)\n",
    "\n",
    "            annotations.append({\"class\": class_name, \"segmentation\": seg})\n",
    "\n",
    "    images.append({\"file_name\": f\"evaluation_{img_id}.tif\", \"annotations\": annotations})\n",
    "\n",
    "with open(submission_save_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\"images\": images}, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepLabV3+ model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepLabModel(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Initialiser DeepLabv3+ fra segmentation_models_pytorch\n",
    "        self.model = smp.create_model(\n",
    "            arch=\"deeplabv3plus\",\n",
    "            encoder_name=\"resnet50\",  # Bruk ResNet-50 som encoder\n",
    "            encoder_weights=\"imagenet\",  # Pre-trent på ImageNet\n",
    "            in_channels=12,  # Tilpasses her til 12-kanals satellittbilder\n",
    "            classes=4,  # Fire klasser\n",
    "        )\n",
    "\n",
    "        # Tapfunksjoner\n",
    "        self.dice_loss_fn = smp.losses.DiceLoss(mode=smp.losses.MULTILABEL_MODE, from_logits=True)\n",
    "        self.bce_loss_fn = smp.losses.SoftBCEWithLogitsLoss()\n",
    "\n",
    "    def forward(self, image):\n",
    "        return self.model(image)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, masks = batch[\"image\"], batch[\"mask\"]\n",
    "        logits = self(images)\n",
    "        loss = self.dice_loss_fn(logits, masks) + self.bce_loss_fn(logits, masks)\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images, masks = batch[\"image\"], batch[\"mask\"]\n",
    "        logits = self(images)\n",
    "        loss = self.dice_loss_fn(logits, masks) + self.bce_loss_fn(logits, masks)\n",
    "        self.log(\"val_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Datasettklasser: Forberedt til å lese bilder (train_images) og masker (train_masks).\n",
    "Augmentering: Anvendt på treningsdata for å forbedre generalisering.\n",
    "Her vil jeg dele ogsa opp data i batcher for trening og validering.\n",
    "'''\n",
    "# Augmentations for training set\n",
    "deeplab_augmentations = albu.Compose(\n",
    "    [\n",
    "        albu.ShiftScaleRotate(\n",
    "            p=0.5,\n",
    "            shift_limit=0.0625,\n",
    "            scale_limit=0.1,\n",
    "            rotate_limit=15,\n",
    "            border_mode=0,\n",
    "            value=0,\n",
    "            mask_value=0,\n",
    "            interpolation=2,\n",
    "        ),\n",
    "        albu.RandomCrop(\n",
    "            p=1,\n",
    "            width=512,\n",
    "            height=512,\n",
    "        ),\n",
    "        albu.HorizontalFlip(p=0.5),\n",
    "        albu.VerticalFlip(p=0.5),\n",
    "        albu.Transpose(p=0.5),\n",
    "        albu.RandomRotate90(p=0.5),\n",
    "        albu.RandomBrightnessContrast(p=0.2),  # Ny\n",
    "        albu.RandomGamma(p=0.2),               # Ny\n",
    "        albu.GaussNoise(p=0.1),                # Ny\n",
    "        albu.MotionBlur(p=0.1),                # Ny\n",
    "        albu.ElasticTransform(p=0.2),          # Ny\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    TrainValDataset(data_root, train_indices, augmentations=deeplab_augmentations),\n",
    "    batch_size=2,\n",
    "    num_workers=0,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    TrainValDataset(data_root, val_indices, augmentations=None),\n",
    "    batch_size=2,\n",
    "    num_workers=0,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Tapsfunksjon: Kombinasjon av Dice Loss og Binary Cross Entropy. Velger AdamW som optimizer. \n",
    "'''\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-2)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50) # Bruker en kosinuskurve til å redusere læringsraten gradvis. God for lange treningsløp.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DeepLabModel()\n",
    "trainer = Trainer(\n",
    "    max_epochs=1, # 100\n",
    "    precision=16,  # Mixed precision for faster training\n",
    "    accelerator=\"auto\",  # Use GPU\n",
    "    devices='auto',  # Number of GPUs\n",
    "    strategy=\"auto\",\n",
    "    callbacks=[ModelCheckpoint(\n",
    "        monitor=\"val_loss\", \n",
    "        save_top_k=1), \n",
    "               LearningRateMonitor()],\n",
    "    log_every_n_steps=10,\n",
    ")\n",
    "'''\n",
    "\n",
    "# prepare trainer\n",
    "trainer = Trainer(\n",
    "    max_epochs=epochs,\n",
    "    callbacks = [\n",
    "        # save model with best validation F1 score\n",
    "        ModelCheckpoint(\n",
    "            dirpath=train_output_dir,\n",
    "            filename=\"best_f1_05\",\n",
    "            save_weights_only=True,\n",
    "            save_top_k=1,\n",
    "            monitor=\"val/f1\",\n",
    "            mode=\"max\",\n",
    "            save_last=False,\n",
    "        ),\n",
    "        LearningRateMonitor(logging_interval=\"step\"),\n",
    "    ],\n",
    "    logger=[TensorBoardLogger(train_output_dir, name=None)],\n",
    "    precision=\"16-mixed\",\n",
    "    deterministic=True,\n",
    "    benchmark=False,\n",
    "    sync_batchnorm=False,\n",
    "    check_val_every_n_epoch=1,\n",
    "    default_root_dir=os.getcwd(),\n",
    "    accelerator=\"auto\",\n",
    "    devices='auto',\n",
    "    strategy=\"auto\",\n",
    "    log_every_n_steps=1,\n",
    ")\n",
    "\n",
    "\n",
    "# Start training\n",
    "trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"test_preds/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader):\n",
    "        images = batch[\"image\"].to(\"cuda\")\n",
    "        file_names = batch[\"image_path\"]  # Hent filnavnene\n",
    "\n",
    "        model_output = model(images)  # Logits\n",
    "        prob_mask = torch.sigmoid(model_output).cpu().numpy()  # Konverter til numpy\n",
    "        \n",
    "        # Lagre sannsynlighetsmaskene i `.npy`-format\n",
    "        for i, file_name in enumerate(file_names):\n",
    "            file_name = os.path.basename(file_name).replace(\".tif\", \".npy\")  \n",
    "            np.save(os.path.join(output_dir, file_name), prob_mask[i].astype(np.float16))\n",
    "\n",
    "\n",
    "# Run evualiation\n",
    "\n",
    "model.load_state_dict(torch.load(train_output_dir / \"best_f1_05.ckpt\")[\"state_dict\"])\n",
    "model = model.cuda()\n",
    "model.eval()\n",
    "\n",
    "val_pred_dir = data_root / \"val_preds\"\n",
    "run_inference(model, val_loader, val_pred_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
