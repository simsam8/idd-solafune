{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook is a part of motokimura's baseline solution for the [Solafune Identifying Deforestation Drivers competition](https://solafune.com/competitions/68ad4759-4686-4bb3-94b8-7063f755b43d?menu=about&tab=overview).\n",
    "See https://github.com/motokimura/solafune_deforestation_baseline for the complete code.\n",
    "\n",
    "> cf. @solafune (https://solafune.com) Use for any purpose other than participation in the competition or commercial use is prohibited. If you would like to use them for any of the above purposes, please contact us.\n",
    "\n",
    "### Description\n",
    "\n",
    "**By running this notebook, you will achieve a score of around 0.533 on the public leaderboard.**\n",
    "\n",
    "This notebook trains a U-Net model for 4-class segmentation (`grassland_shrubland`, `logging`, `mining`, and `plantation`) and generates a submission JSON file for the evaluation images from the output from the U-Net model.\n",
    "\n",
    "The submission JSON file is saved to `data/submission.json`.\n",
    "\n",
    "Before running this notebook, you have to run `generate_masks.ipynb` to generate `.npy` files used for training\n",
    "(`generate_masks.ipynb` is available from https://github.com/motokimura/solafune_deforestation_baseline).\n",
    "\n",
    "### Requirements\n",
    "\n",
    "#### Datasets\n",
    "\n",
    "Organize the dataset as follows:\n",
    "\n",
    "```\n",
    "data/\n",
    "├── evaluation_images/\n",
    "│   ├── evaluation_0.tif\n",
    "│   ├── evaluation_1.tif\n",
    "│   ├── evaluation_2.tif\n",
    "│   ├── ...\n",
    "├── train_images/\n",
    "│   ├── train_0.tif\n",
    "│   ├── train_1.tif\n",
    "│   ├── train_2.tif\n",
    "│   ├── ...\n",
    "├── train_masks/\n",
    "│   ├── train_0.npy\n",
    "│   ├── train_1.npy\n",
    "│   ├── train_2.npy\n",
    "│   ├── ...\n",
    "```\n",
    "\n",
    "`evaluation_images` and `train_images` can be downloaded from the competition page.\n",
    "\n",
    "`train_masks` can be generated by running `generate_masks.ipynb` available from https://github.com/motokimura/solafune_deforestation_baseline.\n",
    "\n",
    "#### Libraries\n",
    "\n",
    "Please install the python packages imported the cell below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/khalilibrahim/master_in_machine_learning/inf367a/idd-solafune/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/khalilibrahim/master_in_machine_learning/inf367a/idd-solafune/.venv/lib/python3.11/site-packages/albumentations/__init__.py:28: UserWarning: A new version of Albumentations is available: '2.0.3' (you have '2.0.1'). Upgrade using: pip install -U albumentations. To disable automatic update checks, set the environment variable NO_ALBUMENTATIONS_UPDATE to 1.\n",
      "  check_for_updates()\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import albumentations as albu  # tested with 1.4.24\n",
    "import imagecodecs\n",
    "import numpy as np  # tested with 1.26.4\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl  # tested with 2.5.0.post0\n",
    "import segmentation_models_pytorch as smp  # tested with 0.3.4\n",
    "import sklearn\n",
    "import tensorboard\n",
    "import tifffile\n",
    "import timm  # tested with 0.9.7\n",
    "import torch  # tested with 2.5.1\n",
    "\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "from rasterio import features\n",
    "from shapely.geometry import Polygon, shape\n",
    "from skimage import measure\n",
    "from timm.optim import create_optimizer_v2\n",
    "from timm.scheduler import create_scheduler_v2\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = Path(\"./data\")\n",
    "\n",
    "class_names = [\"grassland_shrubland\", \"logging\", \"mining\", \"plantation\"]\n",
    "\n",
    "epochs = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define dataset class to load images and masks for training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mask(mask_path):\n",
    "    mask = np.load(mask_path)  # (4, H, W), uint8\n",
    "    assert mask.shape == (4, 1024, 1024)\n",
    "    mask = mask.transpose(1, 2, 0)  # (H, W, 4)\n",
    "    return mask.astype(np.float32) / 255.0  # normalize to [0, 1]\n",
    "\n",
    "\n",
    "def load_image(image_path):\n",
    "    image = tifffile.imread(image_path)  # (H, W, 12), float64\n",
    "    assert image.shape == (1024, 1024, 12)\n",
    "    image = np.nan_to_num(image)  # replace NaN with 0\n",
    "    return image.astype(np.float32)\n",
    "\n",
    "\n",
    "def normalize_image(image):\n",
    "    # mean of train images\n",
    "    mean = np.array(\n",
    "        [\n",
    "            285.8190561180765,\n",
    "            327.22091430696577,\n",
    "            552.9305957826701,\n",
    "            392.1575148484924,\n",
    "            914.3138803812591,\n",
    "            2346.1184507500043,\n",
    "            2884.4831706095824,\n",
    "            2886.442429854111,\n",
    "            3176.7501338557763,\n",
    "            3156.934442092072,\n",
    "            1727.1940075511282,\n",
    "            848.573373995044,\n",
    "        ],\n",
    "        dtype=np.float32\n",
    "    )\n",
    "\n",
    "    # std of train images\n",
    "    std = np.array(\n",
    "        [\n",
    "            216.44975668759372,\n",
    "            269.8880248304874,\n",
    "            309.92790753407064,\n",
    "            397.45655590699,\n",
    "            400.22078920482215,\n",
    "            630.3269651264278,\n",
    "            789.8006920468097,\n",
    "            810.4773696969773,\n",
    "            852.9031432100967,\n",
    "            807.5976198303886,\n",
    "            631.7808113929271,\n",
    "            502.66788721341396,\n",
    "        ],\n",
    "        dtype=np.float32\n",
    "    )\n",
    "    \n",
    "    mean = mean.reshape(12, 1, 1)\n",
    "    std = std.reshape(12, 1, 1)\n",
    "\n",
    "    return (image - mean) / std\n",
    "\n",
    "\n",
    "class TrainValDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_root, sample_indices, augmentations=None):\n",
    "        self.image_paths, self.mask_paths = [], []\n",
    "        for i in sample_indices:\n",
    "            self.image_paths.append(data_root / \"train_images\" / f\"train_{i}.tif\")\n",
    "            self.mask_paths.append(data_root / \"train_masks\" / f\"train_{i}.npy\")\n",
    "        self.augmentations = augmentations\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = {\n",
    "            \"image\": load_image(self.image_paths[idx]),\n",
    "            \"mask\": load_mask(self.mask_paths[idx]),\n",
    "        }\n",
    "\n",
    "        if self.augmentations is not None:\n",
    "            sample = self.augmentations(**sample)\n",
    "\n",
    "        sample[\"image\"] = sample[\"image\"].transpose(2, 0, 1)  # (12, H, W)\n",
    "        sample[\"mask\"] = sample[\"mask\"].transpose(2, 0, 1)  # (4, H, W)\n",
    "\n",
    "        sample[\"image\"] = normalize_image(sample[\"image\"])\n",
    "\n",
    "        # add metadata\n",
    "        sample[\"image_path\"] = str(self.image_paths[idx])\n",
    "        sample[\"mask_path\"] = str(self.mask_paths[idx])\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define U-Net model using pytorch-lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # prepare segmentation model\n",
    "        self.model = smp.create_model(\n",
    "            arch=\"unet\",\n",
    "            encoder_name=\"tu-tf_efficientnetv2_s\",  # use `tf_efficientnetv2_s` from timm\n",
    "            encoder_weights=\"imagenet\",  # always starts from imagenet pre-trained weight\n",
    "            in_channels=12,\n",
    "            classes=4,\n",
    "        )\n",
    "\n",
    "        # prepare loss functions\n",
    "        self.dice_loss_fn = smp.losses.DiceLoss(mode=smp.losses.MULTILABEL_MODE, from_logits=True)\n",
    "        self.bce_loss_fn = smp.losses.SoftBCEWithLogitsLoss(smooth_factor=0.0)\n",
    "\n",
    "        self.training_step_outputs = []\n",
    "        self.validation_step_outputs = []\n",
    "    \n",
    "    def forward(self, image):\n",
    "        # assuming image is already normalized\n",
    "        return self.model(image)  # logits\n",
    "\n",
    "    def shared_step(self, batch, stage):\n",
    "        image = batch[\"image\"]\n",
    "        mask = batch[\"mask\"]\n",
    "\n",
    "        logits_mask = self.forward(image)\n",
    "\n",
    "        loss = self.dice_loss_fn(logits_mask, mask) + self.bce_loss_fn(logits_mask, mask)\n",
    "\n",
    "        # count tp, fp, fn, tn for each class to compute validation metrics at the end of epoch\n",
    "        thresh = 0.5\n",
    "        prob_mask = logits_mask.sigmoid()\n",
    "        tp, fp, fn, tn = smp.metrics.get_stats(\n",
    "            (prob_mask > thresh).long(),\n",
    "            mask.long(),\n",
    "            mode=smp.losses.MULTILABEL_MODE,\n",
    "        )  # each of tp, fp, fn, tn is a tensor of shape (batch_size, num_classes) and of type long\n",
    "\n",
    "        output = {\n",
    "            \"loss\": loss.detach().cpu(),\n",
    "            \"tp\": tp.detach().cpu(),\n",
    "            \"fp\": fp.detach().cpu(),\n",
    "            \"fn\": fn.detach().cpu(),\n",
    "            \"tn\": tn.detach().cpu(),\n",
    "        }\n",
    "        if stage == \"train\":\n",
    "            self.training_step_outputs.append(output)\n",
    "        else:\n",
    "            self.validation_step_outputs.append(output)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        return self.shared_step(batch, \"train\")\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        return self.shared_step(batch, \"val\")\n",
    "\n",
    "    def shared_epoch_end(self, outputs, stage):\n",
    "        def log(name, tensor, prog_bar=False):\n",
    "            self.log(f\"{stage}/{name}\", tensor.to(self.device), sync_dist=True, prog_bar=prog_bar)\n",
    "\n",
    "        # aggregate loss\n",
    "        loss = torch.stack([x[\"loss\"] for x in outputs]).mean()\n",
    "        log(\"loss\", loss, prog_bar=True)\n",
    "\n",
    "        # aggregate tp, fp, fn, tn to compose F1 score for each class\n",
    "        tp = torch.cat([x[\"tp\"] for x in outputs])\n",
    "        fp = torch.cat([x[\"fp\"] for x in outputs])\n",
    "        fn = torch.cat([x[\"fn\"] for x in outputs])\n",
    "        tn = torch.cat([x[\"tn\"] for x in outputs])\n",
    "\n",
    "        f1_scores = {}\n",
    "        for i, class_name in enumerate(class_names):\n",
    "            f1_scores[class_name] = smp.metrics.f1_score(tp[:, i], fp[:, i], fn[:, i], tn[:, i], reduction=\"macro-imagewise\")\n",
    "            log(f\"f1/{class_name}\", f1_scores[class_name], prog_bar=False)\n",
    "\n",
    "        f1_avg = torch.stack([v for v in f1_scores.values()]).mean()\n",
    "        log(\"f1\", f1_avg, prog_bar=True)\n",
    "    \n",
    "    def on_train_epoch_end(self):\n",
    "        self.shared_epoch_end(self.training_step_outputs, \"train\")\n",
    "        self.training_step_outputs.clear()\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        self.shared_epoch_end(self.validation_step_outputs, \"val\")\n",
    "        self.validation_step_outputs.clear()\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        # optimizer\n",
    "        optimizer = create_optimizer_v2(\n",
    "            self.parameters(),\n",
    "            opt=\"adamw\",\n",
    "            lr=1e-4,\n",
    "            weight_decay=1e-2,\n",
    "            filter_bias_and_bn=True,  # filter out bias and batchnorm from weight decay\n",
    "        )\n",
    "\n",
    "        # lr scheduler\n",
    "        scheduler, _ = create_scheduler_v2(\n",
    "            optimizer,\n",
    "            sched=\"cosine\",\n",
    "            num_epochs=epochs,\n",
    "            min_lr=0.0,\n",
    "            warmup_lr=1e-5,\n",
    "            warmup_epochs=0,\n",
    "            warmup_prefix=False,\n",
    "            step_on_epochs=True,\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"interval\": \"epoch\",\n",
    "            },\n",
    "        }\n",
    "\n",
    "    def lr_scheduler_step(self, scheduler, metric):\n",
    "        # workaround for timm's scheduler:\n",
    "        # https://github.com/Lightning-AI/lightning/issues/5555#issuecomment-1065894281\n",
    "        scheduler.step(epoch=self.current_epoch)  # timm's scheduler need the epoch value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare trainer of pytorch-lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ny/1dxctkls3b9g4b_7fjjjk71h0000gn/T/ipykernel_20974/442312327.py:11: UserWarning: Argument 'value' is not valid and will be ignored.\n",
      "  albu.ShiftScaleRotate(\n",
      "/var/folders/ny/1dxctkls3b9g4b_7fjjjk71h0000gn/T/ipykernel_20974/442312327.py:11: UserWarning: Argument 'mask_value' is not valid and will be ignored.\n",
      "  albu.ShiftScaleRotate(\n",
      "/Users/khalilibrahim/master_in_machine_learning/inf367a/idd-solafune/.venv/lib/python3.11/site-packages/albumentations/core/validation.py:45: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
      "  original_init(self, **validated_kwargs)\n",
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "/Users/khalilibrahim/master_in_machine_learning/inf367a/idd-solafune/.venv/lib/python3.11/site-packages/torch/amp/grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Unexpected keys (bn2.bias, bn2.num_batches_tracked, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n"
     ]
    }
   ],
   "source": [
    "train_output_dir = data_root / \"training_result\"\n",
    "\n",
    "# split train_images into train-set and val-set\n",
    "sample_indices = list(range(176))  # train_0.tif to train_175.tif\n",
    "train_indices, val_indices = sklearn.model_selection.train_test_split(sample_indices, test_size=0.2, random_state=42)\n",
    "\n",
    "# augmentations applied only to train-set\n",
    "augmentations = albu.Compose(\n",
    "    [\n",
    "        # shift, scale, and rotate\n",
    "        albu.ShiftScaleRotate(\n",
    "            p=0.5,\n",
    "            shift_limit=0.0625,\n",
    "            scale_limit=0.1,\n",
    "            rotate_limit=15,\n",
    "            border_mode=0,  # constant border\n",
    "            value=0,\n",
    "            mask_value=0,\n",
    "            interpolation=2,  # bicubic\n",
    "        ),\n",
    "        # random crop\n",
    "        albu.RandomCrop(\n",
    "            p=1,\n",
    "            width=512,\n",
    "            height=512,\n",
    "        ),\n",
    "        # flip, transpose, and rotate90\n",
    "        albu.HorizontalFlip(p=0.5),\n",
    "        albu.VerticalFlip(p=0.5),\n",
    "        albu.Transpose(p=0.5),\n",
    "        albu.RandomRotate90(p=0.5),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# prepare data loaders\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    TrainValDataset(\n",
    "        data_root,\n",
    "        train_indices,\n",
    "        augmentations=augmentations,\n",
    "    ),\n",
    "    batch_size=8,\n",
    "    num_workers=8,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    TrainValDataset(\n",
    "        data_root,\n",
    "        val_indices,\n",
    "        augmentations=None,\n",
    "    ),\n",
    "    batch_size=2,\n",
    "    num_workers=8,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "# prepare trainer\n",
    "trainer = Trainer(\n",
    "    max_epochs=epochs,\n",
    "    callbacks = [\n",
    "        # save model with best validation F1 score\n",
    "        ModelCheckpoint(\n",
    "            dirpath=train_output_dir,\n",
    "            filename=\"best_f1_05\",\n",
    "            save_weights_only=True,\n",
    "            save_top_k=1,\n",
    "            monitor=\"val/f1\",\n",
    "            mode=\"max\",\n",
    "            save_last=False,\n",
    "        ),\n",
    "        LearningRateMonitor(logging_interval=\"step\"),\n",
    "    ],\n",
    "    logger=[TensorBoardLogger(train_output_dir, name=None)],\n",
    "    precision=\"16-mixed\",\n",
    "    deterministic=True,\n",
    "    benchmark=False,\n",
    "    sync_batchnorm=False,\n",
    "    check_val_every_n_epoch=5,\n",
    "    default_root_dir=os.getcwd(),\n",
    "    accelerator=\"auto\",\n",
    "    devices='auto',\n",
    "    strategy=\"auto\",\n",
    "    log_every_n_steps=5,\n",
    ")\n",
    "\n",
    "# prepare model\n",
    "model = Model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start training!\n",
    "\n",
    "With the default setting, 10 GB of GPU memory is required. To reduce the memory usage, you can decrease the batch size.\n",
    "\n",
    "The trained model is saved as `data/training_result/best_f1_05.ckpt`.\n",
    "\n",
    "Tensorboard logs are also saved under `data/training_result/version_xx`.\n",
    "\n",
    "**The execution often does not finish even after reaching 200 epochs. In that case, you can stop the execution manually and just proceed to the next cell (do not restart the notebook!).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/khalilibrahim/master_in_machine_learning/inf367a/idd-solafune/.venv/lib/python3.11/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /Users/khalilibrahim/master_in_machine_learning/inf367a/idd-solafune/data/training_result exists and is not empty.\n",
      "\n",
      "  | Name         | Type                  | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0 | model        | Unet                  | 22.1 M | train\n",
      "1 | dice_loss_fn | DiceLoss              | 0      | train\n",
      "2 | bce_loss_fn  | SoftBCEWithLogitsLoss | 0      | train\n",
      "---------------------------------------------------------------\n",
      "22.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "22.1 M    Total params\n",
      "88.383    Total estimated model params size (MB)\n",
      "799       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |                                                                                                                                                                    | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/khalilibrahim/master_in_machine_learning/inf367a/idd-solafune/.venv/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:420: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Users/khalilibrahim/.local/share/uv/python/cpython-3.11.11-macos-aarch64-none/lib/python3.11/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/khalilibrahim/.local/share/uv/python/cpython-3.11.11-macos-aarch64-none/lib/python3.11/multiprocessing/spawn.py\", line 132, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'TrainValDataset' on <module '__main__' (built-in)>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Users/khalilibrahim/.local/share/uv/python/cpython-3.11.11-macos-aarch64-none/lib/python3.11/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/khalilibrahim/.local/share/uv/python/cpython-3.11.11-macos-aarch64-none/lib/python3.11/multiprocessing/spawn.py\", line 132, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'TrainValDataset' on <module '__main__' (built-in)>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Users/khalilibrahim/.local/share/uv/python/cpython-3.11.11-macos-aarch64-none/lib/python3.11/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/khalilibrahim/.local/share/uv/python/cpython-3.11.11-macos-aarch64-none/lib/python3.11/multiprocessing/spawn.py\", line 132, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'TrainValDataset' on <module '__main__' (built-in)>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Users/khalilibrahim/.local/share/uv/python/cpython-3.11.11-macos-aarch64-none/lib/python3.11/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/khalilibrahim/.local/share/uv/python/cpython-3.11.11-macos-aarch64-none/lib/python3.11/multiprocessing/spawn.py\", line 132, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'TrainValDataset' on <module '__main__' (built-in)>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Users/khalilibrahim/.local/share/uv/python/cpython-3.11.11-macos-aarch64-none/lib/python3.11/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/khalilibrahim/.local/share/uv/python/cpython-3.11.11-macos-aarch64-none/lib/python3.11/multiprocessing/spawn.py\", line 132, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'TrainValDataset' on <module '__main__' (built-in)>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Users/khalilibrahim/.local/share/uv/python/cpython-3.11.11-macos-aarch64-none/lib/python3.11/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/khalilibrahim/.local/share/uv/python/cpython-3.11.11-macos-aarch64-none/lib/python3.11/multiprocessing/spawn.py\", line 132, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'TrainValDataset' on <module '__main__' (built-in)>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Users/khalilibrahim/.local/share/uv/python/cpython-3.11.11-macos-aarch64-none/lib/python3.11/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/khalilibrahim/.local/share/uv/python/cpython-3.11.11-macos-aarch64-none/lib/python3.11/multiprocessing/spawn.py\", line 132, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'TrainValDataset' on <module '__main__' (built-in)>\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Users/khalilibrahim/.local/share/uv/python/cpython-3.11.11-macos-aarch64-none/lib/python3.11/multiprocessing/spawn.py\", line 122, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/khalilibrahim/.local/share/uv/python/cpython-3.11.11-macos-aarch64-none/lib/python3.11/multiprocessing/spawn.py\", line 132, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: Can't get attribute 'TrainValDataset' on <module '__main__' (built-in)>\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid(s) 20979) exited unexpectedly",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[0;32m~/master_in_machine_learning/inf367a/idd-solafune/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1243\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1242\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1243\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1244\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n",
      "File \u001b[0;32m~/.local/share/uv/python/cpython-3.11.11-macos-aarch64-none/lib/python3.11/multiprocessing/queues.py:113\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    112\u001b[0m timeout \u001b[38;5;241m=\u001b[39m deadline \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic()\n\u001b[0;32m--> 113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Empty\n",
      "File \u001b[0;32m~/.local/share/uv/python/cpython-3.11.11-macos-aarch64-none/lib/python3.11/multiprocessing/connection.py:257\u001b[0m, in \u001b[0;36m_ConnectionBase.poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_readable()\n\u001b[0;32m--> 257\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_poll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/uv/python/cpython-3.11.11-macos-aarch64-none/lib/python3.11/multiprocessing/connection.py:440\u001b[0m, in \u001b[0;36mConnection._poll\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_poll\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout):\n\u001b[0;32m--> 440\u001b[0m     r \u001b[38;5;241m=\u001b[39m \u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    441\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(r)\n",
      "File \u001b[0;32m~/.local/share/uv/python/cpython-3.11.11-macos-aarch64-none/lib/python3.11/multiprocessing/connection.py:948\u001b[0m, in \u001b[0;36mwait\u001b[0;34m(object_list, timeout)\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 948\u001b[0m     ready \u001b[38;5;241m=\u001b[39m \u001b[43mselector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    949\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ready:\n",
      "File \u001b[0;32m~/.local/share/uv/python/cpython-3.11.11-macos-aarch64-none/lib/python3.11/selectors.py:415\u001b[0m, in \u001b[0;36m_PollLikeSelector.select\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    414\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 415\u001b[0m     fd_event_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_selector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpoll\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mInterruptedError\u001b[39;00m:\n",
      "File \u001b[0;32m~/master_in_machine_learning/inf367a/idd-solafune/.venv/lib/python3.11/site-packages/torch/utils/data/_utils/signal_handling.py:73\u001b[0m, in \u001b[0;36m_set_SIGCHLD_handler.<locals>.handler\u001b[0;34m(signum, frame)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mhandler\u001b[39m(signum, frame):\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;66;03m# This following call uses `waitid` with WNOHANG from C side. Therefore,\u001b[39;00m\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;66;03m# Python can still get and update the process status successfully.\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[43m_error_if_any_worker_fails\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m previous_handler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid 20979) exited unexpectedly with exit code 1. Details are lost due to multiprocessing. Rerunning with num_workers=0 may give better error trace.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# start training\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/master_in_machine_learning/inf367a/idd-solafune/.venv/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:539\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 539\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/master_in_machine_learning/inf367a/idd-solafune/.venv/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:47\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     50\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/master_in_machine_learning/inf367a/idd-solafune/.venv/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:575\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    569\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    571\u001b[0m     ckpt_path,\n\u001b[1;32m    572\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    573\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    574\u001b[0m )\n\u001b[0;32m--> 575\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    578\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/master_in_machine_learning/inf367a/idd-solafune/.venv/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:982\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    977\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_signal_connector\u001b[38;5;241m.\u001b[39mregister_signal_handlers()\n\u001b[1;32m    979\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    981\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 982\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    986\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    987\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: trainer tearing down\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/master_in_machine_learning/inf367a/idd-solafune/.venv/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:1024\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1022\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining:\n\u001b[1;32m   1023\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n\u001b[0;32m-> 1024\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_sanity_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1025\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[1;32m   1026\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_loop\u001b[38;5;241m.\u001b[39mrun()\n",
      "File \u001b[0;32m~/master_in_machine_learning/inf367a/idd-solafune/.venv/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:1053\u001b[0m, in \u001b[0;36mTrainer._run_sanity_check\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1050\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_start\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1052\u001b[0m \u001b[38;5;66;03m# run eval step\u001b[39;00m\n\u001b[0;32m-> 1053\u001b[0m \u001b[43mval_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1055\u001b[0m call\u001b[38;5;241m.\u001b[39m_call_callback_hooks(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mon_sanity_check_end\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1057\u001b[0m \u001b[38;5;66;03m# reset logger connector\u001b[39;00m\n",
      "File \u001b[0;32m~/master_in_machine_learning/inf367a/idd-solafune/.venv/lib/python3.11/site-packages/pytorch_lightning/loops/utilities.py:179\u001b[0m, in \u001b[0;36m_no_grad_context.<locals>._decorator\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    177\u001b[0m     context_manager \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mno_grad\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context_manager():\n\u001b[0;32m--> 179\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mloop_run\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/master_in_machine_learning/inf367a/idd-solafune/.venv/lib/python3.11/site-packages/pytorch_lightning/loops/evaluation_loop.py:137\u001b[0m, in \u001b[0;36m_EvaluationLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    136\u001b[0m     dataloader_iter \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 137\u001b[0m     batch, batch_idx, dataloader_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m previous_dataloader_idx \u001b[38;5;241m!=\u001b[39m dataloader_idx:\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;66;03m# the dataloader has changed, notify the logger connector\u001b[39;00m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_store_dataloader_outputs()\n",
      "File \u001b[0;32m~/master_in_machine_learning/inf367a/idd-solafune/.venv/lib/python3.11/site-packages/pytorch_lightning/loops/fetchers.py:134\u001b[0m, in \u001b[0;36m_PrefetchDataFetcher.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone \u001b[38;5;241m=\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatches\n\u001b[1;32m    132\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone:\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;66;03m# this will run only when no pre-fetching was done.\u001b[39;00m\n\u001b[0;32m--> 134\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__next__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;66;03m# the iterator is empty\u001b[39;00m\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/master_in_machine_learning/inf367a/idd-solafune/.venv/lib/python3.11/site-packages/pytorch_lightning/loops/fetchers.py:61\u001b[0m, in \u001b[0;36m_DataFetcher.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_profiler()\n\u001b[1;32m     60\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 61\u001b[0m     batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m     63\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdone \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/master_in_machine_learning/inf367a/idd-solafune/.venv/lib/python3.11/site-packages/pytorch_lightning/utilities/combined_loader.py:341\u001b[0m, in \u001b[0;36mCombinedLoader.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m _ITERATOR_RETURN:\n\u001b[1;32m    340\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 341\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_iterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    342\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator, _Sequential):\n\u001b[1;32m    343\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/master_in_machine_learning/inf367a/idd-solafune/.venv/lib/python3.11/site-packages/pytorch_lightning/utilities/combined_loader.py:142\u001b[0m, in \u001b[0;36m_Sequential.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 142\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miterators\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;66;03m# try the next iterator\u001b[39;00m\n\u001b[1;32m    145\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_next_iterator()\n",
      "File \u001b[0;32m~/master_in_machine_learning/inf367a/idd-solafune/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m~/master_in_machine_learning/inf367a/idd-solafune/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1448\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1445\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1447\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1448\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1449\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1450\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1451\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/master_in_machine_learning/inf367a/idd-solafune/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1412\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1408\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1409\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1410\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1411\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1412\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1413\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1414\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/master_in_machine_learning/inf367a/idd-solafune/.venv/lib/python3.11/site-packages/torch/utils/data/dataloader.py:1256\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1254\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(failed_workers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1255\u001b[0m     pids_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mstr\u001b[39m(w\u001b[38;5;241m.\u001b[39mpid) \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m failed_workers)\n\u001b[0;32m-> 1256\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   1257\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataLoader worker (pid(s) \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpids_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) exited unexpectedly\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1258\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m   1259\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, queue\u001b[38;5;241m.\u001b[39mEmpty):\n\u001b[1;32m   1260\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid(s) 20979) exited unexpectedly"
     ]
    }
   ],
   "source": [
    "# start training\n",
    "trainer.fit(\n",
    "    model,\n",
    "    train_dataloaders=train_loader,\n",
    "    val_dataloaders=val_loader,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute the evaluation metric for the validation set\n",
    "\n",
    "Before predicting the evaluation images, let's predict the validation set and compute the evaluation metric.\n",
    "\n",
    "This may be useful to check if the model is trained properly and to tune the parameters for post-processing (e.g., score threshold, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_inference(model, loader, pred_output_dir):\n",
    "    pred_output_dir = Path(pred_output_dir)\n",
    "    pred_output_dir.mkdir(exist_ok=True, parents=True)\n",
    "\n",
    "    for batch in tqdm(loader):\n",
    "        img = batch[\"image\"].cuda()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits_mask = model(img)\n",
    "            prob_mask = logits_mask.sigmoid()\n",
    "\n",
    "        # save prob mask as numpy array\n",
    "        for i in range(img.size(0)):\n",
    "            file_name = os.path.basename(batch[\"image_path\"][i])\n",
    "            prob_mask_i = prob_mask[i].cpu().numpy()  # (4, 1024, 1024)\n",
    "\n",
    "            np.save(\n",
    "                pred_output_dir / file_name.replace(\".tif\", \".npy\"),\n",
    "                prob_mask_i.astype(np.float16),\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best checkpoint and run inference on val-set\n",
    "del model\n",
    "\n",
    "model = Model()\n",
    "model.load_state_dict(torch.load(train_output_dir / \"best_f1_05.ckpt\")[\"state_dict\"])\n",
    "model = model.cuda()\n",
    "model.eval()\n",
    "\n",
    "val_pred_dir = data_root / \"val_preds\"\n",
    "run_inference(model, val_loader, val_pred_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_f1_score(pred_mask, truth_mask):\n",
    "    # `pred_mask` is a binary numpy array of shape (H, W) = (1024, 1024)\n",
    "    # `truth_mask` is a binaru numpy array of shape (H, W) = (1024, 1024)\n",
    "    assert pred_mask.shape == (1024, 1024), f\"{pred_mask.shape=}\"\n",
    "    assert truth_mask.shape == (1024, 1024), f\"{truth_mask.shape=}\"\n",
    "\n",
    "    tp = ((pred_mask > 0) & (truth_mask > 0)).sum()\n",
    "    fp = ((pred_mask > 0) & (truth_mask == 0)).sum()\n",
    "    fn = ((pred_mask == 0) & (truth_mask > 0)).sum()\n",
    "    precision = tp / (tp + fp) if tp + fp > 0 else 1  # if no prediction, precision is considered as 1\n",
    "    recall = tp / (tp + fn) if tp + fn > 0 else 1  # if no ground truth, recall is considered as 1\n",
    "    f1 = 2 * precision * recall / (precision + recall) if precision + recall > 0 else 0  # if either precision or recall is 0, f1 is 0\n",
    "\n",
    "    return f1\n",
    "\n",
    "\n",
    "score_thresh = 0.5  # threshold to binarize the prediction mask\n",
    "min_area = 10000  # if the predicted area of a class is less than this, submit an zero mask because small predicted areas are often false positives\n",
    "\n",
    "val_f1_scores = {}\n",
    "for idx in sorted(val_indices):\n",
    "    fn = f\"train_{idx}\"\n",
    "    # prepare prediction mask\n",
    "    pred_mask = np.load(val_pred_dir / f\"{fn}.npy\")  # (4, 1024, 1024)\n",
    "    pred_mask = pred_mask > score_thresh  # binarize\n",
    "    # prepare ground truth mask\n",
    "    truth_mask = np.load(data_root / \"train_masks\" / f\"{fn}.npy\")  # (4, 1024, 1024)\n",
    "    # compute f1 score for each class\n",
    "    val_f1_scores[fn] = {}\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        pred_for_a_class = pred_mask[i]\n",
    "        if pred_for_a_class.sum() < min_area:\n",
    "            pred_for_a_class = np.zeros_like(pred_for_a_class)  # set all to zero if the predicted area is less than `min_area`\n",
    "        val_f1_scores[fn][class_name] = compute_f1_score(pred_for_a_class, truth_mask[i])\n",
    "val_f1_scores = pd.DataFrame(val_f1_scores).T\n",
    "\n",
    "# add a column for average of all the 4 classes\n",
    "val_f1_scores[\"all_classes\"] = val_f1_scores.mean(axis=1)\n",
    "# add a row for average of all the val images\n",
    "val_f1_scores.loc[\"all_images\"] = val_f1_scores.mean()\n",
    "\n",
    "print(f\"val f1 score: {val_f1_scores.loc['all_images', 'all_classes']:.4f}\")\n",
    "\n",
    "val_f1_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict the evaluation images and generate a submission JSON file\n",
    "\n",
    "Let's predict the evaluation images as already done with the validation set, and generate a submission JSON file.\n",
    "\n",
    "The submission JSON file will be saved as `data/submission.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_root):\n",
    "        self.image_paths = []\n",
    "        for i in range(118):  # evaluation_0.tif to evaluation_117.tif\n",
    "            self.image_paths.append(data_root / \"evaluation_images\" / f\"evaluation_{i}.tif\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = {\n",
    "            \"image\": load_image(self.image_paths[idx]),\n",
    "        }\n",
    "\n",
    "        sample[\"image\"] = sample[\"image\"].transpose(2, 0, 1)  # (12, H, W)\n",
    "        sample[\"image\"] = normalize_image(sample[\"image\"])\n",
    "\n",
    "        # add metadata\n",
    "        sample[\"image_path\"] = str(self.image_paths[idx])\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(\n",
    "    TestDataset(data_root),\n",
    "    batch_size=4,\n",
    "    num_workers=8,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "test_pred_dir = data_root / \"test_preds\"\n",
    "run_inference(model, test_loader, test_pred_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`detect_polygons()` below extracts isolated areas as polygons from the predicted mask.\n",
    "\n",
    "The point is `min_area` parameter to filter out small areas. Small predicted areas are often false positives which decrease the evaluation score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_polygons(pred_dir, score_thresh, min_area):\n",
    "    pred_dir = Path(pred_dir)\n",
    "    pred_paths = list(pred_dir.glob(\"*.npy\"))\n",
    "    pred_paths = sorted(pred_paths)\n",
    "\n",
    "    polygons_all_imgs = {}\n",
    "    for pred_path in tqdm(pred_paths):\n",
    "        polygons_all_classes = {}\n",
    "\n",
    "        mask = np.load(pred_path)  # (4, 1024, 1024)\n",
    "        mask = mask > score_thresh  # binarize\n",
    "        for i, class_name in enumerate(class_names):\n",
    "            mask_for_a_class = mask[i]\n",
    "            if mask_for_a_class.sum() < min_area:\n",
    "                mask_for_a_class = np.zeros_like(mask_for_a_class)  # set all to zero if the predicted area is less than `min_area`\n",
    "\n",
    "            # extract polygons from the binarized mask\n",
    "            label = measure.label(mask_for_a_class, connectivity=2, background=0).astype(np.uint8)\n",
    "            polygons = []\n",
    "            for p, value in features.shapes(label, label):\n",
    "                p = shape(p).buffer(0.5)\n",
    "                p = p.simplify(tolerance=0.5)\n",
    "                polygons.append(p)\n",
    "            polygons_all_classes[class_name] = polygons\n",
    "        polygons_all_imgs[pred_path.name.replace(\".npy\", \".tif\")] = polygons_all_classes\n",
    "\n",
    "    return polygons_all_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred_polygons = detect_polygons(test_pred_dir, score_thresh=score_thresh, min_area=min_area)\n",
    "\n",
    "submission_save_path = data_root / f\"submission.json\"\n",
    "\n",
    "images = []\n",
    "for img_id in range(118):  # evaluation_0.tif to evaluation_117.tif\n",
    "    annotations = []\n",
    "    for class_name in class_names:\n",
    "        for poly in test_pred_polygons[f\"evaluation_{img_id}.tif\"][class_name]:\n",
    "            seg: list[float] = []  # [x0, y0, x1, y1, ..., xN, yN]\n",
    "            for xy in poly.exterior.coords:\n",
    "                seg.extend(xy)\n",
    "\n",
    "            annotations.append({\"class\": class_name, \"segmentation\": seg})\n",
    "\n",
    "    images.append({\"file_name\": f\"evaluation_{img_id}.tif\", \"annotations\": annotations})\n",
    "\n",
    "with open(submission_save_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump({\"images\": images}, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepLabV3+ model Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepLabModel(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # Initialiser DeepLabv3+ fra segmentation_models_pytorch\n",
    "        self.model = smp.create_model(\n",
    "            arch=\"deeplabv3plus\",\n",
    "            encoder_name=\"resnet50\",  # Bruk ResNet-50 som encoder\n",
    "            encoder_weights=\"imagenet\",  # Pre-trent på ImageNet\n",
    "            in_channels=12,  # Tilpasses her til 12-kanals satellittbilder\n",
    "            classes=4, # Fire klasser       \n",
    "            #encoder_output_stride=16\n",
    "        )\n",
    "\n",
    "        # Tapfunksjoner\n",
    "        self.dice_loss_fn = smp.losses.DiceLoss(mode=smp.losses.MULTILABEL_MODE, from_logits=True)\n",
    "        self.bce_loss_fn = smp.losses.SoftBCEWithLogitsLoss()\n",
    "\n",
    "    def forward(self, image):\n",
    "        print('Size: ', image.size())\n",
    "        return self.model(image)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, masks = batch[\"image\"], batch[\"mask\"]\n",
    "        logits = self(images)\n",
    "        loss = self.dice_loss_fn(logits, masks) + self.bce_loss_fn(logits, masks)\n",
    "        self.log(\"train_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images, masks = batch[\"image\"], batch[\"mask\"]\n",
    "        logits = self(images)\n",
    "        loss = self.dice_loss_fn(logits, masks) + self.bce_loss_fn(logits, masks)\n",
    "        self.log(\"val_loss\", loss, on_step=True, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters(), lr=1e-4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ny/1dxctkls3b9g4b_7fjjjk71h0000gn/T/ipykernel_20974/1281807332.py:9: UserWarning: Argument 'value' is not valid and will be ignored.\n",
      "  albu.ShiftScaleRotate(\n",
      "/var/folders/ny/1dxctkls3b9g4b_7fjjjk71h0000gn/T/ipykernel_20974/1281807332.py:9: UserWarning: Argument 'mask_value' is not valid and will be ignored.\n",
      "  albu.ShiftScaleRotate(\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Datasettklasser: Forberedt til å lese bilder (train_images) og masker (train_masks).\n",
    "Augmentering: Anvendt på treningsdata for å forbedre generalisering.\n",
    "Her vil jeg dele ogsa opp data i batcher for trening og validering.\n",
    "'''\n",
    "# Augmentations for training set\n",
    "deeplab_augmentations = albu.Compose(\n",
    "    [\n",
    "        albu.ShiftScaleRotate(\n",
    "            p=0.5,\n",
    "            shift_limit=0.0625,\n",
    "            scale_limit=0.1,\n",
    "            rotate_limit=15,\n",
    "            border_mode=0,\n",
    "            value=0,\n",
    "            mask_value=0,\n",
    "            interpolation=2,\n",
    "        ),\n",
    "        albu.RandomCrop(\n",
    "            p=1,\n",
    "            width=512,\n",
    "            height=512,\n",
    "        ),\n",
    "        albu.HorizontalFlip(p=0.5),\n",
    "        albu.VerticalFlip(p=0.5),\n",
    "        albu.Transpose(p=0.5),\n",
    "        albu.RandomRotate90(p=0.5),\n",
    "        albu.RandomBrightnessContrast(p=0.2),  # Ny\n",
    "        albu.RandomGamma(p=0.2),               # Ny\n",
    "        albu.GaussNoise(p=0.1),                # Ny\n",
    "        albu.MotionBlur(p=0.1),                # Ny\n",
    "        albu.ElasticTransform(p=0.2),          # Ny\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    TrainValDataset(data_root, train_indices, augmentations=deeplab_augmentations),\n",
    "    batch_size=2,\n",
    "    num_workers=0,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    TrainValDataset(data_root, val_indices, augmentations=None),\n",
    "    batch_size=2,\n",
    "    num_workers=0,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Tapsfunksjon: Kombinasjon av Dice Loss og Binary Cross Entropy. Velger AdamW som optimizer. \n",
    "'''\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-2)\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=50) # Bruker en kosinuskurve til å redusere læringsraten gradvis. God for lange treningsløp.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 16bit Automatic Mixed Precision (AMP)\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name         | Type                  | Params | Mode \n",
      "---------------------------------------------------------------\n",
      "0 | model        | DeepLabV3Plus         | 26.7 M | train\n",
      "1 | dice_loss_fn | DiceLoss              | 0      | train\n",
      "2 | bce_loss_fn  | SoftBCEWithLogitsLoss | 0      | train\n",
      "---------------------------------------------------------------\n",
      "26.7 M    Trainable params\n",
      "0         Non-trainable params\n",
      "26.7 M    Total params\n",
      "106.826   Total estimated model params size (MB)\n",
      "209       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size:  torch.Size([2, 12, 1024, 1024])                                                                                                                                                | 0/2 [00:00<?, ?it/s]\n",
      "Size:  torch.Size([2, 12, 1024, 1024])██████████████████████████████████████████████████████████████████▌                                                                     | 1/2 [00:12<00:12,  0.08it/s]\n",
      "Size:  torch.Size([2, 12, 512, 512])                                                                                                                                                 | 0/70 [00:00<?, ?it/s]\n",
      "Size:  torch.Size([2, 12, 512, 512])                                                                                                                                | 1/70 [00:15<17:24,  0.07it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])                                                                                                                                | 2/70 [00:37<21:06,  0.05it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])                                                                                                                                | 3/70 [00:53<19:55,  0.06it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])                                                                                                                                | 4/70 [01:14<20:37,  0.05it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])                                                                                                                                | 5/70 [02:00<26:00,  0.04it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])                                                                                                                                | 6/70 [02:22<25:17,  0.04it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])                                                                                                                                | 7/70 [02:41<24:09,  0.04it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])                                                                                                                                | 8/70 [03:03<23:43,  0.04it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])                                                                                                                                | 9/70 [03:40<24:55,  0.04it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])                                                                                                                               | 10/70 [04:11<25:11,  0.04it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])█▍                                                                                                                             | 11/70 [04:55<26:26,  0.04it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])███▌                                                                                                                           | 12/70 [05:23<26:02,  0.04it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])█████▋                                                                                                                         | 13/70 [05:57<26:09,  0.04it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])███████▊                                                                                                                       | 14/70 [06:24<25:36,  0.04it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])█████████▉                                                                                                                     | 15/70 [06:52<25:13,  0.04it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])████████████                                                                                                                   | 16/70 [07:14<24:25,  0.04it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])██████████████▏                                                                                                                | 17/70 [07:31<23:28,  0.04it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])████████████████▎                                                                                                              | 18/70 [07:48<22:32,  0.04it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])██████████████████▍                                                                                                            | 19/70 [08:01<21:33,  0.04it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])████████████████████▌                                                                                                          | 20/70 [08:26<21:05,  0.04it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])██████████████████████▋                                                                                                        | 21/70 [08:41<20:17,  0.04it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])████████████████████████▊                                                                                                      | 22/70 [09:12<20:04,  0.04it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])██████████████████████████▉                                                                                                    | 23/70 [09:30<19:26,  0.04it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])█████████████████████████████                                                                                                  | 24/70 [09:48<18:48,  0.04it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])███████████████████████████████▏                                                                                               | 25/70 [10:21<18:39,  0.04it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])█████████████████████████████████▎                                                                                             | 26/70 [10:39<18:02,  0.04it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])███████████████████████████████████▍                                                                                           | 27/70 [11:02<17:35,  0.04it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])█████████████████████████████████████▌                                                                                         | 28/70 [11:29<17:13,  0.04it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])███████████████████████████████████████▋                                                                                       | 29/70 [11:56<16:53,  0.04it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])█████████████████████████████████████████▊                                                                                     | 30/70 [12:15<16:20,  0.04it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])███████████████████████████████████████████▉                                                                                   | 31/70 [12:33<15:48,  0.04it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])██████████████████████████████████████████████                                                                                 | 32/70 [12:53<15:18,  0.04it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])████████████████████████████████████████████████▏                                                                              | 33/70 [13:19<14:56,  0.04it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])██████████████████████████████████████████████████▎                                                                            | 34/70 [13:41<14:29,  0.04it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])████████████████████████████████████████████████████▌                                                                          | 35/70 [14:08<14:08,  0.04it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])██████████████████████████████████████████████████████▋                                                                        | 36/70 [14:35<13:46,  0.04it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])████████████████████████████████████████████████████████▊                                                                      | 37/70 [15:05<13:27,  0.04it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])██████████████████████████████████████████████████████████▉                                                                    | 38/70 [15:39<13:10,  0.04it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])█████████████████████████████████████████████████████████████                                                                  | 39/70 [16:07<12:48,  0.04it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])███████████████████████████████████████████████████████████████▏                                                               | 40/70 [16:25<12:19,  0.04it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])█████████████████████████████████████████████████████████████████▎                                                             | 41/70 [16:45<11:51,  0.04it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])███████████████████████████████████████████████████████████████████▍                                                           | 42/70 [17:04<11:23,  0.04it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])█████████████████████████████████████████████████████████████████████▌                                                         | 43/70 [17:34<11:01,  0.04it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])███████████████████████████████████████████████████████████████████████▋                                                       | 44/70 [17:52<10:33,  0.04it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])█████████████████████████████████████████████████████████████████████████▊                                                     | 45/70 [18:12<10:07,  0.04it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])███████████████████████████████████████████████████████████████████████████▉                                                   | 46/70 [18:31<09:39,  0.04it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])██████████████████████████████████████████████████████████████████████████████                                                 | 47/70 [18:53<09:14,  0.04it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])████████████████████████████████████████████████████████████████████████████████▏                                              | 48/70 [19:08<08:46,  0.04it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])██████████████████████████████████████████████████████████████████████████████████▎                                            | 49/70 [19:46<08:28,  0.04it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])████████████████████████████████████████████████████████████████████████████████████▍                                          | 50/70 [21:58<08:47,  0.04it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])██████████████████████████████████████████████████████████████████████████████████████▌                                        | 51/70 [22:37<08:25,  0.04it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])████████████████████████████████████████████████████████████████████████████████████████▋                                      | 52/70 [22:57<07:56,  0.04it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])██████████████████████████████████████████████████████████████████████████████████████████▊                                    | 53/70 [23:19<07:28,  0.04it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])████████████████████████████████████████████████████████████████████████████████████████████▉                                  | 54/70 [23:47<07:03,  0.04it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])███████████████████████████████████████████████████████████████████████████████████████████████                                | 55/70 [24:16<06:37,  0.04it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])█████████████████████████████████████████████████████████████████████████████████████████████████▏                             | 56/70 [24:36<06:09,  0.04it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])███████████████████████████████████████████████████████████████████████████████████████████████████▎                           | 57/70 [24:49<05:39,  0.04it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])█████████████████████████████████████████████████████████████████████████████████████████████████████▍                         | 58/70 [25:06<05:11,  0.04it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])███████████████████████████████████████████████████████████████████████████████████████████████████████▌                       | 59/70 [25:21<04:43,  0.04it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])█████████████████████████████████████████████████████████████████████████████████████████████████████████▋                     | 60/70 [25:37<04:16,  0.04it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])███████████████████████████████████████████████████████████████████████████████████████████████████████████▊                   | 61/70 [25:51<03:48,  0.04it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])█████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                 | 62/70 [26:11<03:22,  0.04it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])████████████████████████████████████████████████████████████████████████████████████████████████████████████████               | 63/70 [26:26<02:56,  0.04it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])██████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏            | 64/70 [26:39<02:29,  0.04it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎          | 65/70 [26:51<02:03,  0.04it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍        | 66/70 [27:02<01:38,  0.04it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌      | 67/70 [27:16<01:13,  0.04it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋    | 68/70 [27:36<00:48,  0.04it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊  | 69/70 [27:52<00:24,  0.04it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])                                                                                                                                        | 0/70 [00:00<?, ?it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])                                                                                                                                | 1/70 [00:17<20:29,  0.06it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])                                                                                                                                | 2/70 [00:37<21:15,  0.05it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])                                                                                                                                | 3/70 [00:51<19:20,  0.06it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])                                                                                                                                | 4/70 [01:09<19:13,  0.06it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])                                                                                                                                | 5/70 [01:24<18:16,  0.06it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])                                                                                                                                | 6/70 [01:36<17:12,  0.06it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])                                                                                                                                | 7/70 [01:52<16:50,  0.06it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])                                                                                                                                | 8/70 [02:13<17:13,  0.06it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])                                                                                                                                | 9/70 [02:37<17:46,  0.06it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])                                                                                                                               | 10/70 [02:57<17:42,  0.06it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])█▍                                                                                                                             | 11/70 [03:15<17:26,  0.06it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])███▌                                                                                                                           | 12/70 [03:29<16:50,  0.06it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])█████▋                                                                                                                         | 13/70 [03:42<16:14,  0.06it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])███████▊                                                                                                                       | 14/70 [03:58<15:55,  0.06it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])█████████▉                                                                                                                     | 15/70 [04:17<15:44,  0.06it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])████████████                                                                                                                   | 16/70 [04:34<15:27,  0.06it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])██████████████▏                                                                                                                | 17/70 [04:59<15:33,  0.06it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])████████████████▎                                                                                                              | 18/70 [05:11<14:59,  0.06it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])██████████████████▍                                                                                                            | 19/70 [05:27<14:39,  0.06it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])████████████████████▌                                                                                                          | 20/70 [05:40<14:11,  0.06it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])██████████████████████▋                                                                                                        | 21/70 [06:01<14:02,  0.06it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])████████████████████████▊                                                                                                      | 22/70 [06:17<13:42,  0.06it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])██████████████████████████▉                                                                                                    | 23/70 [06:33<13:23,  0.06it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])█████████████████████████████                                                                                                  | 24/70 [06:47<13:00,  0.06it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])███████████████████████████████▏                                                                                               | 25/70 [06:59<12:35,  0.06it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])█████████████████████████████████▎                                                                                             | 26/70 [07:12<12:12,  0.06it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])███████████████████████████████████▍                                                                                           | 27/70 [07:34<12:03,  0.06it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])█████████████████████████████████████▌                                                                                         | 28/70 [07:52<11:48,  0.06it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])███████████████████████████████████████▋                                                                                       | 29/70 [08:06<11:28,  0.06it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])█████████████████████████████████████████▊                                                                                     | 30/70 [08:20<11:06,  0.06it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])███████████████████████████████████████████▉                                                                                   | 31/70 [08:35<10:48,  0.06it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])██████████████████████████████████████████████                                                                                 | 32/70 [08:59<10:40,  0.06it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])████████████████████████████████████████████████▏                                                                              | 33/70 [09:13<10:20,  0.06it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])██████████████████████████████████████████████████▎                                                                            | 34/70 [09:25<09:58,  0.06it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])████████████████████████████████████████████████████▌                                                                          | 35/70 [09:37<09:37,  0.06it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])██████████████████████████████████████████████████████▋                                                                        | 36/70 [09:56<09:23,  0.06it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])████████████████████████████████████████████████████████▊                                                                      | 37/70 [10:23<09:16,  0.06it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])██████████████████████████████████████████████████████████▉                                                                    | 38/70 [10:40<08:59,  0.06it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])█████████████████████████████████████████████████████████████                                                                  | 39/70 [10:55<08:41,  0.06it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])███████████████████████████████████████████████████████████████▏                                                               | 40/70 [11:11<08:23,  0.06it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])█████████████████████████████████████████████████████████████████▎                                                             | 41/70 [11:25<08:04,  0.06it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])███████████████████████████████████████████████████████████████████▍                                                           | 42/70 [11:39<07:46,  0.06it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])█████████████████████████████████████████████████████████████████████▌                                                         | 43/70 [11:51<07:26,  0.06it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])███████████████████████████████████████████████████████████████████████▋                                                       | 44/70 [12:04<07:07,  0.06it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])█████████████████████████████████████████████████████████████████████████▊                                                     | 45/70 [12:14<06:48,  0.06it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])███████████████████████████████████████████████████████████████████████████▉                                                   | 46/70 [12:27<06:29,  0.06it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])██████████████████████████████████████████████████████████████████████████████                                                 | 47/70 [13:57<06:49,  0.06it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])████████████████████████████████████████████████████████████████████████████████▏                                              | 48/70 [14:36<06:41,  0.05it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])██████████████████████████████████████████████████████████████████████████████████▎                                            | 49/70 [15:11<06:30,  0.05it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])████████████████████████████████████████████████████████████████████████████████████▍                                          | 50/70 [16:05<06:26,  0.05it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])██████████████████████████████████████████████████████████████████████████████████████▌                                        | 51/70 [16:31<06:09,  0.05it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])████████████████████████████████████████████████████████████████████████████████████████▋                                      | 52/70 [17:01<05:53,  0.05it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])██████████████████████████████████████████████████████████████████████████████████████████▊                                    | 53/70 [17:16<05:32,  0.05it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])████████████████████████████████████████████████████████████████████████████████████████████▉                                  | 54/70 [17:34<05:12,  0.05it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])███████████████████████████████████████████████████████████████████████████████████████████████                                | 55/70 [18:02<04:55,  0.05it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])█████████████████████████████████████████████████████████████████████████████████████████████████▏                             | 56/70 [18:24<04:36,  0.05it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])███████████████████████████████████████████████████████████████████████████████████████████████████▎                           | 57/70 [18:43<04:16,  0.05it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])█████████████████████████████████████████████████████████████████████████████████████████████████████▍                         | 58/70 [19:01<03:56,  0.05it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])███████████████████████████████████████████████████████████████████████████████████████████████████████▌                       | 59/70 [19:26<03:37,  0.05it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])█████████████████████████████████████████████████████████████████████████████████████████████████████████▋                     | 60/70 [19:48<03:18,  0.05it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])███████████████████████████████████████████████████████████████████████████████████████████████████████████▊                   | 61/70 [20:13<02:59,  0.05it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])█████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                 | 62/70 [20:35<02:39,  0.05it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])████████████████████████████████████████████████████████████████████████████████████████████████████████████████               | 63/70 [20:54<02:19,  0.05it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])██████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏            | 64/70 [21:14<01:59,  0.05it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎          | 65/70 [21:42<01:40,  0.05it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍        | 66/70 [22:03<01:20,  0.05it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌      | 67/70 [22:20<01:00,  0.05it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋    | 68/70 [22:44<00:40,  0.05it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊  | 69/70 [23:14<00:20,  0.05it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])                                                                                                                                        | 0/70 [00:00<?, ?it/s, v_num=9]\n",
      "Size:  torch.Size([2, 12, 512, 512])                                                                                                                                | 1/70 [00:17<20:30,  0.06it/s, v_num=9]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m~/master_in_machine_learning/inf367a/idd-solafune/.venv/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:47\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n",
      "File \u001b[0;32m~/master_in_machine_learning/inf367a/idd-solafune/.venv/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:575\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    569\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    571\u001b[0m     ckpt_path,\n\u001b[1;32m    572\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    573\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    574\u001b[0m )\n\u001b[0;32m--> 575\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n",
      "File \u001b[0;32m~/master_in_machine_learning/inf367a/idd-solafune/.venv/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:982\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    979\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    980\u001b[0m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[1;32m    981\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[0;32m--> 982\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[1;32m    985\u001b[0m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[1;32m    986\u001b[0m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n",
      "File \u001b[0;32m~/master_in_machine_learning/inf367a/idd-solafune/.venv/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:1026\u001b[0m, in \u001b[0;36mTrainer._run_stage\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mset_detect_anomaly(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_detect_anomaly):\n\u001b[0;32m-> 1026\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/master_in_machine_learning/inf367a/idd-solafune/.venv/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:216\u001b[0m, in \u001b[0;36m_FitLoop.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_start()\n\u001b[0;32m--> 216\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end()\n",
      "File \u001b[0;32m~/master_in_machine_learning/inf367a/idd-solafune/.venv/lib/python3.11/site-packages/pytorch_lightning/loops/fit_loop.py:455\u001b[0m, in \u001b[0;36m_FitLoop.advance\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 455\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepoch_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/master_in_machine_learning/inf367a/idd-solafune/.venv/lib/python3.11/site-packages/pytorch_lightning/loops/training_epoch_loop.py:150\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.run\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 150\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mon_advance_end(data_fetcher)\n",
      "File \u001b[0;32m~/master_in_machine_learning/inf367a/idd-solafune/.venv/lib/python3.11/site-packages/pytorch_lightning/loops/training_epoch_loop.py:320\u001b[0m, in \u001b[0;36m_TrainingEpochLoop.advance\u001b[0;34m(self, data_fetcher)\u001b[0m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mlightning_module\u001b[38;5;241m.\u001b[39mautomatic_optimization:\n\u001b[1;32m    319\u001b[0m     \u001b[38;5;66;03m# in automatic optimization, there can only be one optimizer\u001b[39;00m\n\u001b[0;32m--> 320\u001b[0m     batch_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautomatic_optimization\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/master_in_machine_learning/inf367a/idd-solafune/.venv/lib/python3.11/site-packages/pytorch_lightning/loops/optimization/automatic.py:192\u001b[0m, in \u001b[0;36m_AutomaticOptimization.run\u001b[0;34m(self, optimizer, batch_idx, kwargs)\u001b[0m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;66;03m# BACKWARD PASS\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;66;03m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 192\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    194\u001b[0m result \u001b[38;5;241m=\u001b[39m closure\u001b[38;5;241m.\u001b[39mconsume_result()\n",
      "File \u001b[0;32m~/master_in_machine_learning/inf367a/idd-solafune/.venv/lib/python3.11/site-packages/pytorch_lightning/loops/optimization/automatic.py:270\u001b[0m, in \u001b[0;36m_AutomaticOptimization._optimizer_step\u001b[0;34m(self, batch_idx, train_step_and_backward_closure)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;66;03m# model hook\u001b[39;00m\n\u001b[0;32m--> 270\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_lightning_module_hook\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moptimizer_step\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    275\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    276\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_step_and_backward_closure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    277\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m should_accumulate:\n",
      "File \u001b[0;32m~/master_in_machine_learning/inf367a/idd-solafune/.venv/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:171\u001b[0m, in \u001b[0;36m_call_lightning_module_hook\u001b[0;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 171\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n",
      "File \u001b[0;32m~/master_in_machine_learning/inf367a/idd-solafune/.venv/lib/python3.11/site-packages/pytorch_lightning/core/module.py:1302\u001b[0m, in \u001b[0;36mLightningModule.optimizer_step\u001b[0;34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001b[0m\n\u001b[1;32m   1278\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"Override this method to adjust the default way the :class:`~pytorch_lightning.trainer.trainer.Trainer` calls\u001b[39;00m\n\u001b[1;32m   1279\u001b[0m \u001b[38;5;124;03mthe optimizer.\u001b[39;00m\n\u001b[1;32m   1280\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1300\u001b[0m \n\u001b[1;32m   1301\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m-> 1302\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptimizer_closure\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/master_in_machine_learning/inf367a/idd-solafune/.venv/lib/python3.11/site-packages/pytorch_lightning/core/optimizer.py:154\u001b[0m, in \u001b[0;36mLightningOptimizer.step\u001b[0;34m(self, closure, **kwargs)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_strategy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 154\u001b[0m step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_strategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_on_after_step()\n",
      "File \u001b[0;32m~/master_in_machine_learning/inf367a/idd-solafune/.venv/lib/python3.11/site-packages/pytorch_lightning/strategies/strategy.py:239\u001b[0m, in \u001b[0;36mStrategy.optimizer_step\u001b[0;34m(self, optimizer, closure, model, **kwargs)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, pl\u001b[38;5;241m.\u001b[39mLightningModule)\n\u001b[0;32m--> 239\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprecision_plugin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/master_in_machine_learning/inf367a/idd-solafune/.venv/lib/python3.11/site-packages/pytorch_lightning/plugins/precision/amp.py:79\u001b[0m, in \u001b[0;36mMixedPrecision.optimizer_step\u001b[0;34m(self, optimizer, model, closure, **kwargs)\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m MisconfigurationException(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAMP and the LBFGS optimizer are not compatible.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 79\u001b[0m closure_result \u001b[38;5;241m=\u001b[39m \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;66;03m# If backward was skipped in automatic optimization (return None), unscaling is not needed\u001b[39;00m\n",
      "File \u001b[0;32m~/master_in_machine_learning/inf367a/idd-solafune/.venv/lib/python3.11/site-packages/pytorch_lightning/loops/optimization/automatic.py:146\u001b[0m, in \u001b[0;36mClosure.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[Tensor]:\n\u001b[0;32m--> 146\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    147\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\u001b[38;5;241m.\u001b[39mloss\n",
      "File \u001b[0;32m~/master_in_machine_learning/inf367a/idd-solafune/.venv/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/master_in_machine_learning/inf367a/idd-solafune/.venv/lib/python3.11/site-packages/pytorch_lightning/loops/optimization/automatic.py:131\u001b[0m, in \u001b[0;36mClosure.closure\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m    129\u001b[0m \u001b[38;5;129m@torch\u001b[39m\u001b[38;5;241m.\u001b[39menable_grad()\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mclosure\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ClosureResult:\n\u001b[0;32m--> 131\u001b[0m     step_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_step_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m step_output\u001b[38;5;241m.\u001b[39mclosure_loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/master_in_machine_learning/inf367a/idd-solafune/.venv/lib/python3.11/site-packages/pytorch_lightning/loops/optimization/automatic.py:319\u001b[0m, in \u001b[0;36m_AutomaticOptimization._training_step\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m trainer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\n\u001b[0;32m--> 319\u001b[0m training_step_output \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtraining_step\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mpost_training_step()  \u001b[38;5;66;03m# unused hook - call anyway for backward compatibility\u001b[39;00m\n",
      "File \u001b[0;32m~/master_in_machine_learning/inf367a/idd-solafune/.venv/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:323\u001b[0m, in \u001b[0;36m_call_strategy_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 323\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n",
      "File \u001b[0;32m~/master_in_machine_learning/inf367a/idd-solafune/.venv/lib/python3.11/site-packages/pytorch_lightning/strategies/strategy.py:391\u001b[0m, in \u001b[0;36mStrategy.training_step\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    390\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_redirection(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_step\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 391\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[38], line 25\u001b[0m, in \u001b[0;36mDeepLabModel.training_step\u001b[0;34m(self, batch, batch_idx)\u001b[0m\n\u001b[1;32m     24\u001b[0m images, masks \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m\"\u001b[39m], batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmask\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m---> 25\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdice_loss_fn(logits, masks) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbce_loss_fn(logits, masks)\n",
      "File \u001b[0;32m~/master_in_machine_learning/inf367a/idd-solafune/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/master_in_machine_learning/inf367a/idd-solafune/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[38], line 21\u001b[0m, in \u001b[0;36mDeepLabModel.forward\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSize: \u001b[39m\u001b[38;5;124m'\u001b[39m, image\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/master_in_machine_learning/inf367a/idd-solafune/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/master_in_machine_learning/inf367a/idd-solafune/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/master_in_machine_learning/inf367a/idd-solafune/.venv/lib/python3.11/site-packages/segmentation_models_pytorch/base/model.py:48\u001b[0m, in \u001b[0;36mSegmentationModel.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_input_shape(x)\n\u001b[0;32m---> 48\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m decoder_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(\u001b[38;5;241m*\u001b[39mfeatures)\n",
      "File \u001b[0;32m~/master_in_machine_learning/inf367a/idd-solafune/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/master_in_machine_learning/inf367a/idd-solafune/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/master_in_machine_learning/inf367a/idd-solafune/.venv/lib/python3.11/site-packages/segmentation_models_pytorch/encoders/resnet.py:63\u001b[0m, in \u001b[0;36mResNetEncoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_depth \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m---> 63\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mstages\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m     features\u001b[38;5;241m.\u001b[39mappend(x)\n",
      "File \u001b[0;32m~/master_in_machine_learning/inf367a/idd-solafune/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/master_in_machine_learning/inf367a/idd-solafune/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/master_in_machine_learning/inf367a/idd-solafune/.venv/lib/python3.11/site-packages/torch/nn/modules/container.py:250\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 250\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/master_in_machine_learning/inf367a/idd-solafune/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/master_in_machine_learning/inf367a/idd-solafune/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/master_in_machine_learning/inf367a/idd-solafune/.venv/lib/python3.11/site-packages/torchvision/models/resnet.py:147\u001b[0m, in \u001b[0;36mBottleneck.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    146\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv1(x)\n\u001b[0;32m--> 147\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbn1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n",
      "File \u001b[0;32m~/master_in_machine_learning/inf367a/idd-solafune/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/master_in_machine_learning/inf367a/idd-solafune/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/master_in_machine_learning/inf367a/idd-solafune/.venv/lib/python3.11/site-packages/torch/nn/modules/batchnorm.py:193\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    196\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/master_in_machine_learning/inf367a/idd-solafune/.venv/lib/python3.11/site-packages/torch/nn/functional.py:2812\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2810\u001b[0m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[0;32m-> 2812\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2813\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2814\u001b[0m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2815\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2816\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2817\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2818\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2819\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2820\u001b[0m \u001b[43m    \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2821\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2822\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 48\u001b[0m\n\u001b[1;32m     18\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     19\u001b[0m     max_epochs\u001b[38;5;241m=\u001b[39mepochs,\n\u001b[1;32m     20\u001b[0m     callbacks \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     43\u001b[0m     log_every_n_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,\n\u001b[1;32m     44\u001b[0m )\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Start training\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     50\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m~/master_in_machine_learning/inf367a/idd-solafune/.venv/lib/python3.11/site-packages/pytorch_lightning/trainer/trainer.py:539\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 539\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/master_in_machine_learning/inf367a/idd-solafune/.venv/lib/python3.11/site-packages/pytorch_lightning/trainer/call.py:64\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(launcher, _SubprocessScriptLauncher):\n\u001b[1;32m     63\u001b[0m         launcher\u001b[38;5;241m.\u001b[39mkill(_get_sigkill_signal())\n\u001b[0;32m---> 64\u001b[0m     \u001b[43mexit\u001b[49m(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[1;32m     67\u001b[0m     _interrupt(trainer, exception)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'exit' is not defined"
     ]
    }
   ],
   "source": [
    "model = DeepLabModel()\n",
    "'''\n",
    "trainer = Trainer(\n",
    "    max_epochs=1, # 100\n",
    "    precision=16,  # Mixed precision for faster training\n",
    "    accelerator=\"auto\",  # Use GPU\n",
    "    devices='auto',  # Number of GPUs\n",
    "    strategy=\"auto\",\n",
    "    callbacks=[ModelCheckpoint(\n",
    "        monitor=\"val_loss\", \n",
    "        save_top_k=1), \n",
    "               LearningRateMonitor()],\n",
    "    log_every_n_steps=10,\n",
    ")\n",
    "'''\n",
    "\n",
    "# prepare trainer\n",
    "trainer = Trainer(\n",
    "    max_epochs=epochs,\n",
    "    callbacks = [\n",
    "        # save model with best validation F1 score\n",
    "        ModelCheckpoint(\n",
    "            dirpath=train_output_dir,\n",
    "            filename=\"best_f1_05\",\n",
    "            save_weights_only=True,\n",
    "            save_top_k=1,\n",
    "            monitor=\"val/f1\",\n",
    "            mode=\"max\",\n",
    "            save_last=False,\n",
    "        ),\n",
    "        LearningRateMonitor(logging_interval=\"step\"),\n",
    "    ],\n",
    "    logger=[TensorBoardLogger(train_output_dir, name=None)],\n",
    "    precision=\"16-mixed\",\n",
    "    deterministic=True,\n",
    "    benchmark=False,\n",
    "    sync_batchnorm=False,\n",
    "    check_val_every_n_epoch=1,\n",
    "    default_root_dir=os.getcwd(),\n",
    "    accelerator=\"auto\",\n",
    "    devices='auto',\n",
    "    strategy=\"auto\",\n",
    "    log_every_n_steps=1,\n",
    ")\n",
    "\n",
    "\n",
    "# Start training\n",
    "trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"test_preds/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(test_loader):\n",
    "        images = batch[\"image\"].to(\"cuda\")\n",
    "        file_names = batch[\"image_path\"]  # Hent filnavnene\n",
    "\n",
    "        model_output = model(images)  # Logits\n",
    "        prob_mask = torch.sigmoid(model_output).cpu().numpy()  # Konverter til numpy\n",
    "        \n",
    "        # Lagre sannsynlighetsmaskene i `.npy`-format\n",
    "        for i, file_name in enumerate(file_names):\n",
    "            file_name = os.path.basename(file_name).replace(\".tif\", \".npy\")  \n",
    "            np.save(os.path.join(output_dir, file_name), prob_mask[i].astype(np.float16))\n",
    "\n",
    "\n",
    "# Run evualiation\n",
    "\n",
    "model.load_state_dict(torch.load(train_output_dir / \"best_f1_05.ckpt\")[\"state_dict\"])\n",
    "model = model.cuda()\n",
    "model.eval()\n",
    "\n",
    "val_pred_dir = data_root / \"val_preds\"\n",
    "run_inference(model, val_loader, val_pred_dir)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
